# Neural Image Captioning with Visual Attention

## Project Overview

This repository contains our final report for a project conducted as part of the Neural Networks and Deep Learning course. The project, developed in collaboration with Lucia Hainline, explores image captioning using neural networks with visual attention, inspired by the paper:

“Show, Attend and Tell: Neural Image Caption Generation with Visual Attention” by Xu et al. (2014).

Due to restrictions on sharing our GitHub repository publicly, we have only made our final report publicly available in this repository as a PDF document.

About the Project

Our implementation follows the key ideas from Xu et al. (2014), incorporating:
- Convolutional Neural Networks (CNNs) for image feature extraction
- Recurrent Neural Networks (RNNs) / LSTMs for sequence generation
- Soft Visual Attention Mechanisms to dynamically focus on relevant image regions while generating captions

## Final Report

The final report provides a detailed explanation of our methodology, model architecture, training process, and results. It is available as: Project_Caption_Generation.pdf

### Authors

👩‍💻 Lucia Hainline
👩‍💻 Anouck Rietveld
